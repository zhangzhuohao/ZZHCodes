{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3901b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25019\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\statsmodels\\compat\\pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6724a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.launch_dlc()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee4a2621",
   "metadata": {},
   "source": [
    "# Create a new DLC project\n",
    "\n",
    "deeplabcut.create_new_project('Name of the project', 'Name of the experimenter', ['Full path of video 1', 'Full path of video2', 'Full path of video3'], working_directory='Full path of the working directory', copy_videos=True/False, multianimal=True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01720fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiani\\OneDrive\\00_Work\\03_Projects\\DLCTracking\\DLCTrackingModels\\Claire\\Models\n",
      "C:\\Users\\jiani\\OneDrive\\00_Work\\03_Projects\\DLCTracking\\DLCTrackingModels\\Claire\\Models\\ClaireOpto-CJM-2022-05-12\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "name_of_the_project = 'ClaireOpto'\n",
    "experimenter = 'CJM'\n",
    "video_path = 'C:\\\\Users\\\\jiani\\\\OneDrive\\\\00_Work\\\\03_Projects\\\\DLCTracking\\\\DLCTrackingModels\\\\Claire\\\\Videos'\n",
    "working_path = 'C:\\\\Users\\\\jiani\\\\OneDrive\\\\00_Work\\\\03_Projects\\\\DLCTracking\\\\DLCTrackingModels\\\\Claire\\\\Models'\n",
    "\n",
    "#path_config_file = deeplabcut.create_new_project(name_of_the_project, experimenter,\n",
    "#                              [video_path], working_directory= working_path, copy_videos=True, multianimal=False)\n",
    "print(working_path)\n",
    "path_config_file = 'C:\\\\Users\\\\jiani\\\\OneDrive\\\\00_Work\\\\03_Projects\\\\DLCTracking\\\\DLCTrackingModels\\\\Claire\\\\Models\\\\ClaireOpto-CJM-2022-05-12'\n",
    "\n",
    "print(path_config_file)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9324300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiani\\OneDrive\\00_Work\\03_Projects\\DLCTracking\\DLCTrackingModels\\Claire\\Models\\ClaireOpto-CJM-2022-05-12\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "path_config_file = os.path.join(working_path, 'ClaireOpto-CJM-2022-05-12', 'config.yaml')\n",
    "print(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2939f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 29.9  seconds.\n",
      "Extracting and downsampling... 299  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [00:07, 41.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 41.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 41.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:06, 44.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 42.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 42.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:06, 43.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 40.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 30.0  seconds.\n",
      "Extracting and downsampling... 300  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:07, 39.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Frame extraction\n",
    "'''\n",
    "deeplabcut.extract_frames?\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "deeplabcut.extract_frames(path_config_file, mode='automatic', algo='kmeans', userfeedback=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3add1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label extracted frames\n",
    "\n",
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.launch_dlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1bd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiani\\OneDrive\\00_Work\\03_Projects\\DLCTracking\\DLCTrackingModels\\Claire\\Models\\ClaireOpto-CJM-2022-05-12\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b01d2684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by CJM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:38<00:00,  1.62it/s]\n",
      "100%|██████████| 39/39 [00:23<00:00,  1.68it/s]\n",
      "100%|██████████| 26/26 [00:15<00:00,  1.65it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.65it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.65it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.72it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.62it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.71it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.61it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.65it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.70it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003119a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# build skeleton\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSkeletonBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\skeleton.py:56\u001b[0m, in \u001b[0;36mSkeletonBuilder.__init__\u001b[1;34m(self, config_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m     51\u001b[0m     folder\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcropped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabeled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m ):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_hdf(\n\u001b[0;32m     54\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectedData_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscorer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m     )\n\u001b[1;32m---> 56\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpick_labeled_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividuals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnames:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mxs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividuals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\skeleton.py:108\u001b[0m, in \u001b[0;36mSkeletonBuilder.pick_labeled_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m kept \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mstack()\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m    107\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(kept)\n\u001b[1;32m--> 108\u001b[0m row, col \u001b[38;5;241m=\u001b[39m kept\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row, col\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227d4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jiani\\\\OneDrive\\\\00_Work\\\\03_Projects\\\\DLCTracking\\\\DLCTrackingModels\\\\Claire\\\\Models\\\\ClaireOpto-CJM-2022-05-12\\\\labeled-data\\\\Claire_2022-01-17_Trial014\\\\img055.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create training data set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_shuffles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py:865\u001b[0m, in \u001b[0;36mcreate_training_dataset\u001b[1;34m(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, augmenter_type)\u001b[0m\n\u001b[0;32m    855\u001b[0m (\n\u001b[0;32m    856\u001b[0m     datafilename,\n\u001b[0;32m    857\u001b[0m     metadatafilename,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m=\u001b[39m auxiliaryfunctions\u001b[38;5;241m.\u001b[39mGetDataandMetaDataFilenames(\n\u001b[0;32m    859\u001b[0m     trainingsetfolder, trainFraction, shuffle, cfg\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# Saving data file (convert to training file for deeper cut (*.mat))\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m data, MatlabData \u001b[38;5;241m=\u001b[39m \u001b[43mformat_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainIndices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbodyparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_path\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m sio\u001b[38;5;241m.\u001b[39msavemat(\n\u001b[0;32m    869\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_path, datafilename), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: MatlabData}\n\u001b[0;32m    870\u001b[0m )\n\u001b[0;32m    872\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;66;03m# Saving metadata (Pickle file)\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py:631\u001b[0m, in \u001b[0;36mformat_training_data\u001b[1;34m(df, train_inds, nbodyparts, project_path)\u001b[0m\n\u001b[0;32m    629\u001b[0m filename \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex[i]\n\u001b[0;32m    630\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 631\u001b[0m img_shape \u001b[38;5;241m=\u001b[39m \u001b[43mread_image_shape_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m img_shape\n\u001b[0;32m    633\u001b[0m temp \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py:613\u001b[0m, in \u001b[0;36mread_image_shape_fast\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_image_shape_fast\u001b[39m(path):\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;66;03m# Blazing fast and does not load the image into memory\u001b[39;00m\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m    614\u001b[0m         width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mgetbands()), height, width\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\PIL\\Image.py:3068\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3065\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3068\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3069\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jiani\\\\OneDrive\\\\00_Work\\\\03_Projects\\\\DLCTracking\\\\DLCTrackingModels\\\\Claire\\\\Models\\\\ClaireOpto-CJM-2022-05-12\\\\labeled-data\\\\Claire_2022-01-17_Trial014\\\\img055.png'"
     ]
    }
   ],
   "source": [
    "# create training data set\n",
    "deeplabcut.create_training_dataset(path_config_file, num_shuffles=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb74ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions (do not edit)\n",
    "Task: ClaireOpto\n",
    "scorer: CJM\n",
    "date: May12\n",
    "\n",
    "# Project path (change when moving around)\n",
    "project_path: /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/\n",
    "\n",
    "# Annotation data set configuration (and individual video cropping parameters)\n",
    "video_sets: \n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial006.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial013.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial014.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial021.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial048.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial050.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial060.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial078.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial087.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial095.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial101.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial122.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial188.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "  /content/drive/My Drive/ClaireOpto-CJM-2022-05-12/videos/Claire_2022-01-17_Trial195.avi: \n",
    "   crop: 0, 832, 0, 747\n",
    "\n",
    "bodyparts:\n",
    "- L_finger5\n",
    "- L_finger4\n",
    "- L_hand\n",
    "- R_finger2\n",
    "- R_finger3\n",
    "- R_hand\n",
    "- EyeL\n",
    "- EarL\n",
    "- Nose\n",
    "\n",
    "start: 0\n",
    "stop: 1\n",
    "numframes2pick: 20\n",
    "\n",
    "# Plotting configuration\n",
    "skeleton:\n",
    "skeleton_color: blue\n",
    "pcutoff: 0.4\n",
    "dotsize: 12\n",
    "alphavalue: 0.7\n",
    "colormap: jet\n",
    "\n",
    "# Training,Evaluation and Analysis configuration\n",
    "TrainingFraction:\n",
    "- 0.95\n",
    "iteration: 0\n",
    "resnet:\n",
    "snapshotindex: -1\n",
    "batch_size: 8\n",
    "\n",
    "# Cropping Parameters (for analysis and outlier frame detection)\n",
    "cropping: false\n",
    "#if cropping is true for analysis, then set the values here:\n",
    "x1: 0\n",
    "x2: 640\n",
    "y1: 277\n",
    "y2: 624\n",
    "\n",
    "# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)\n",
    "corner2move2:\n",
    "- 50\n",
    "- 50\n",
    "move2corner: true\n",
    "multianimalproject: false\n",
    "identity:\n",
    "default_net_type: resnet_50\n",
    "default_augmenter: default\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
